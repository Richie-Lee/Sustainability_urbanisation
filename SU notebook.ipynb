{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa99dc2",
   "metadata": {},
   "source": [
    "# ABN Amro - Sustainability Urbanisation Case\n",
    "\n",
    "---\n",
    "\n",
    "## Outline\n",
    "- Part 0: Import raw data (\n",
    "- Part 1: Exploratory analysis (Summary statistics, Outliers, Segmentation, Trend/Seasonality)\n",
    "- Part 2: Feature engineering (Parsing, Scaling, Missing data, Seasonality)\n",
    "- Part 3: Model specification (OLS, LASSO, Random Forest)\n",
    "- Part 4: Performance evaluation\n",
    "\n",
    "**Note:** for practical reasons, I recommend working with a subsample of the data. Total runtime of the notebook is kept light to enable easy/timely evaluation in case any users would be interested in looking under the hood.\n",
    "\n",
    "---\n",
    "# Part 0: Importing Data & Specify filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "# Set the style & colors for the plots\n",
    "sns.set_style('darkgrid')\n",
    "_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c661b",
   "metadata": {},
   "source": [
    "**Specify Filepath following the example below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = \"C:/Users/richi/Downloads/Case_data/\" # Change here\n",
    "\n",
    "bm_full = pd.read_csv(data_filepath + \"building_metadata.csv\") # dim (1_449, 6)\n",
    "bmr_full = pd.read_csv(data_filepath + \"building_meter_readings.csv\") # dim (20_216_100, 4)\n",
    "wd_full = pd.read_csv(data_filepath + \"weather_data.csv\") # dim (139_773, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a64a8",
   "metadata": {},
   "source": [
    "Merge tables & random sample subset of observations for development convenience\n",
    "- **Specify subsample (*n*) here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363cbe1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 100000\n",
    "\n",
    "bm = bm_full.sample(min(len(bm_full), n), random_state = 0)\n",
    "bmr = bmr_full.sample(min(len(bmr_full), n), random_state = 0)\n",
    "wd = wd_full.sample(min(len(wd_full), n), random_state = 0)\n",
    "\n",
    "# Merge tables (some rows may get lost - remaining rows (after loss) are displayed print statement at the end)\n",
    "merged_df = bm.merge(bmr, on='building_id', how='inner')\n",
    "df = merged_df.merge(wd, on=['site_id', 'timestamp'], how='inner')\n",
    "\n",
    "# save original dataset\n",
    "raw_data_merged = df.copy()\n",
    "\n",
    "print(df.shape, df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365ccdf",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7fcdd0",
   "metadata": {},
   "source": [
    "## 1.1 Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous variables\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a824b9ed",
   "metadata": {},
   "source": [
    "## 1.2 Evaluating distributions & handling (multivariate) outliers\n",
    "Plot target variable ``meter_reading`` distributions:\n",
    "- significant **outliers identified through visual inspection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9918d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df[\"meter_reading\"])\n",
    "plt.title(\"meter reading\")\n",
    "plt.show()\n",
    " \n",
    "sns.kdeplot(df[\"meter_reading\"], label = \"meter reading\")\n",
    "plt.axvline(x = 2002, color = \"black\", linestyle = \"--\", linewidth = \"0.6\", label = \"90% quantile\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e113cc",
   "metadata": {},
   "source": [
    "Inspect the outliers after aggregating data per ```building_id``` (main VOI: ```meter_reading```):\n",
    "- Univariate outliers per observation / building\n",
    "- Correlation between meter_reading & other *building characteristics* (```square_feet```, ```floor_count```, ```year_built```, ```air_temperature```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the dataframe by \"building_id\" and taking the mean\n",
    "df_building_mean = df.groupby('building_id').mean(numeric_only=True).reset_index()\n",
    "df_building_median = df.groupby('building_id').median(numeric_only=True).reset_index()\n",
    "\n",
    "# To highlight the buildings with the highest energy consumption (aggregated by mean/median)\n",
    "df_building_mean.sort_values(by = \"meter_reading\", ascending = False)\n",
    "df_building_median.sort_values(by = \"meter_reading\", ascending = False)\n",
    "df_building_mean[\"meter_reading\"].describe().round()\n",
    "df_building_median[\"meter_reading\"].describe().round()\n",
    "\n",
    "# Use median for more robust insights\n",
    "for building_characteristic in [\"square_feet\", \"floor_count\", \"air_temperature\", \"year_built\"]:\n",
    "    plt.scatter(df_building_median[building_characteristic], df_building_median[\"meter_reading\"], alpha = 0.5, marker = \".\")\n",
    "    # plt.scatter(df_building_mean[building_characteristic], df_building_mean[\"meter_reading\"], alpha = 0.5, marker = \".\")\n",
    "    plt.ylabel(\"Meter reading\")\n",
    "    plt.xlabel(building_characteristic)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8027b5",
   "metadata": {},
   "source": [
    "Simple implementation to **remove outliers using interquantile range (IQR) method**, based on the target variable ```meter_reading``` (univariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926aa61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, column_name):\n",
    "    # Calculate Q1, Q3, and IQR\n",
    "    Q1 = df[column_name].quantile(0.10)\n",
    "    Q3 = df[column_name].quantile(0.90)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define bounds\n",
    "    lower_bound = max(0, Q1 - 1.5 * IQR) # truncate at 0, remove negative readings\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]\n",
    "    num_outliers = len(outliers)\n",
    "    \n",
    "    # Remove outliers\n",
    "    df_no_outliers = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]\n",
    "    \n",
    "    # Print actions\n",
    "    print(f\"Number of outliers removed ({column_name}): {num_outliers}/{len(df)} ({round(num_outliers/len(df) * 100, 1)} %)\")\n",
    "    print(f\"Lower and Upper bounds for outliers: ({lower_bound}, {upper_bound})\")\n",
    "    \n",
    "    return df_no_outliers, num_outliers, (lower_bound, upper_bound)\n",
    "\n",
    "df_no_outliers, num_removed, bounds = remove_outliers(df, \"meter_reading\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc43387",
   "metadata": {},
   "source": [
    "Quck check on effect of outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c331340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same plots as above\n",
    "plt.boxplot(df[\"meter_reading\"])\n",
    "plt.title(\"meter reading\")\n",
    "plt.show()\n",
    " \n",
    "sns.kdeplot(df[\"meter_reading\"], label = \"meter reading\")\n",
    "plt.axvline(x = 2002, color = \"black\", linestyle = \"--\", linewidth = \"0.6\", label = \"90% quantile\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Same plots as above\n",
    "plt.boxplot(df_no_outliers[\"meter_reading\"])\n",
    "plt.title(\"meter reading\")\n",
    "plt.show()\n",
    " \n",
    "sns.kdeplot(df_no_outliers[\"meter_reading\"], label = \"meter reading\")\n",
    "plt.axvline(x = 2002, color = \"black\", linestyle = \"--\", linewidth = \"0.6\", label = \"90% quantile\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e685a1",
   "metadata": {},
   "source": [
    "## 1.3 Segmentation & Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad16fe",
   "metadata": {},
   "source": [
    "Check for sensible segmentation of observations by evaluation distributions using the following categorical data:\n",
    "- Measurement device type (```meter```)\n",
    "- Building neighbourhood (```site_id```)\n",
    "- Building's ```primary use```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_for_categories(df, categorical_col, target_col):\n",
    "    \"\"\"\n",
    "    Generates a box plot for a target variable split by categories.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - categorical_col: Column name of the categorical variable\n",
    "    - target_col: Column name of the target variable\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.boxplot(data=df, x=categorical_col, y=target_col)  # 'ax' now holds the axis object\n",
    "    plt.title(f'Boxplot of \"{target_col}\" for each category in \"{categorical_col}\"')\n",
    "    # Rotate Xticks for readability\n",
    "    if rotate_axis == True:\n",
    "        plt.xticks(rotation=45) \n",
    "    # Setting the y-axis limits\n",
    "    ax.set_ylim([lower_bound, upper_bound])\n",
    "    plt.show()\n",
    "\n",
    "# Define your plot bounds (for convenient horizont comparison)\n",
    "lower_bound, upper_bound = 0, 2000\n",
    "\n",
    "# Make boxplots\n",
    "rotate_axis = False\n",
    "boxplot_for_categories(df, 'meter', 'meter_reading') # per meter\n",
    "boxplot_for_categories(df, 'site_id', 'meter_reading') # per site\n",
    "\n",
    "rotate_axis = True\n",
    "boxplot_for_categories(df, 'primary_use', 'meter_reading') # Per primary use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dab5ff",
   "metadata": {},
   "source": [
    "## 1.4 Trends & Seasonality\n",
    "Group data per day (```timestamp```), by aggregating through means and medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9347fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_daily(data, voi):\n",
    "    # Convert the \"timestamp\" column to datetime format if it's not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(data['timestamp']):\n",
    "        data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    \n",
    "    # Set the \"timestamp\" column as the index\n",
    "    data = data.set_index('timestamp')\n",
    "    \n",
    "    # Resample the data to daily frequency and calculate the mean and median\n",
    "    daily_data = data.resample('D').agg({voi: ['mean', 'median']}).reset_index()\n",
    "    \n",
    "    # Flatten multi-level column names\n",
    "    daily_data.columns = ['_'.join(col).rstrip('_') for col in daily_data.columns.values]\n",
    "    \n",
    "    return daily_data\n",
    "\n",
    "df_daily = aggregate_to_daily(df, \"meter_reading\")\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf45ca9",
   "metadata": {},
   "source": [
    "Plot ```meter_reading``` over time though a **moving average** of 7 to account for weekday-seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_trend(data, voi):    \n",
    "    window_size = 7 # week\n",
    "    \n",
    "    # Group by timestamp and take the mean for the given variable of interest (voi)\n",
    "    data_avg = data.groupby(\"timestamp\")[voi].mean().reset_index()\n",
    "    \n",
    "    # Compute the moving average\n",
    "    data_avg['rolling_mean'] = data_avg[voi].rolling(window=window_size).mean()\n",
    "    \n",
    "    # Using the first date in each window for the timestamp\n",
    "    data_avg['rolling_timestamp'] = data_avg[\"timestamp\"].shift(window_size - 1)\n",
    "\n",
    "    # Drop NaN values\n",
    "    data_avg = data_avg.dropna(subset=['rolling_mean', 'rolling_timestamp'])\n",
    "    \n",
    "    x = data_avg['rolling_timestamp']\n",
    "    y = data_avg['rolling_mean']\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y, label=f\"Moving Avg of {voi}\")\n",
    "    \n",
    "    # Formatting for better visualization\n",
    "    plt.gcf().autofmt_xdate()  # Auto-format the x-axis for datetime values\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(f\"Moving Avg\")\n",
    "    plt.title(f\"Moving Average trend over time\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Moving Average (7 days)\")\n",
    "    plt.show()\n",
    "    \n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean & median\n",
    "ma7_mean = rolling_trend(df_daily, \"meter_reading_mean\")\n",
    "ma7_median = rolling_trend(df_daily, \"meter_reading_median\")\n",
    "\n",
    "# Mean after removing outliers\n",
    "df_daily_no_outliers, num_removed, bounds = remove_outliers(df_daily, \"meter_reading_mean\")  \n",
    "ma7_mean_no_outliers = rolling_trend(df_daily_no_outliers, \"meter_reading_mean\")\n",
    "\n",
    "plt.plot(ma7_median[0], ma7_median[1], label = \"median\")\n",
    "plt.plot(ma7_mean_no_outliers[0], ma7_mean_no_outliers[1], label = \"mean (4% outliers removed)\")\n",
    "plt.legend()\n",
    "plt.title(\"Moving average (7)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4cb0ae",
   "metadata": {},
   "source": [
    "**Seasonal Decomposition:**\n",
    "- Decomposing the time series into its constituent parts (trend, seasonality, residuals) can visually indicate seasonality.\n",
    "- **Augmented Dickey-Fuller (ADF) Test for Trends**: This tests the null hypothesis that a unit root is present in a time series sample. A unit root would imply that the time series is non-stationary and has some form of structure (such as a trend).\n",
    "- **Ljung-Box Test for Seasonality**: After the STL decomposition, you can apply the Ljung-Box test on the residuals to check if they are white noise. If they aren't, this can imply the presence of seasonality or some other form of autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ed49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "def decompose_time_series(ts, period=90):\n",
    "    \"\"\"\n",
    "    Decompose a time series and plot its components.\n",
    "    Apply tests for trend and seasonality.\n",
    "    \n",
    "    Parameters:\n",
    "    - ts: Time series data\n",
    "    - period: Seasonal period\n",
    "    \n",
    "    Returns:\n",
    "    - Decomposition result object\n",
    "    \"\"\"\n",
    "    plot_color = _colors[0]\n",
    "    \n",
    "    # Decompose the time series\n",
    "    decomposition = sm.tsa.seasonal_decompose(ts, model='additive', period=period)\n",
    "    \n",
    "    # Plot the components\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.subplot(411)\n",
    "    plt.plot(ts, label='Original', color=\"black\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.subplot(412)\n",
    "    plt.plot(decomposition.trend, label=f'Trend ({period})', color=plot_color)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.subplot(413)\n",
    "    plt.plot(decomposition.seasonal, label=f'Seasonal ({period})', color=plot_color)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.subplot(414)\n",
    "    plt.plot(decomposition.resid, label=f'Residual ({period})', color=plot_color)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ADF Test\n",
    "    adf_result = adfuller(ts)\n",
    "    print(f\"ADF Statistic: {adf_result[0]}\")\n",
    "    print(f\"p-value: {adf_result[1]}\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f\"Critical Value ({key}): {value}\")\n",
    "    \n",
    "    # Assuming the residuals from the decomposition are stored in decomposition.resid\n",
    "    # Ljung-Box Test\n",
    "    lb_test_stat, lb_p_value = acorr_ljungbox(decomposition.resid.dropna(), lags=[period])\n",
    "    print(f\"Ljung-Box Test Statistic: {lb_test_stat[0]}\")\n",
    "    print(f\"p-value: {lb_p_value[0]}\")\n",
    "    \n",
    "    return decomposition\n",
    "\n",
    "# Usage\n",
    "ts = df_daily_no_outliers.set_index('timestamp')['meter_reading_median']\n",
    "decomposition = decompose_time_series(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f854c83",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0619ad",
   "metadata": {},
   "source": [
    "### Binning ``year_built``: parsing *continuous* to *custom categorical* \n",
    "- Pre-2000: Bins of 20 year\n",
    "- Post-2000: Bins of 1 year\n",
    "\n",
    "**Note:**\n",
    "- Other categorical variables (```primary_use```, ```site_id```, ```meter```, ```building_id```, ```season```) are categorised using **one-hot encoding** (see: model specifcation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1247a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_year(year_series):\n",
    "    # Bins and labels for years prior to 2000\n",
    "    bins_1900s = list(range(1900, 2000, 20)) + [2000]\n",
    "    labels_1900s = [f\"{start}-{start+9}\" for start in bins_1900s[:-1]]\n",
    "\n",
    "    # Bins and labels for years from 2000 onwards\n",
    "    bins_2000s = list(range(2000, 2018))\n",
    "    labels_2000s = [str(year) for year in bins_2000s[:-1]]\n",
    "\n",
    "    # Combine bins and labels\n",
    "    bins = bins_1900s + bins_2000s\n",
    "    labels = labels_1900s + labels_2000s\n",
    "\n",
    "    return pd.cut(year_series, bins=bins, labels=labels, right=False,  duplicates='drop')\n",
    "\n",
    "# If year_built = continuous, transform to categorical. Else skip\n",
    "if type(df[\"year_built\"][0]) != str:\n",
    "     df['year_built'] = categorize_year(df['year_built'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbb0c4",
   "metadata": {},
   "source": [
    "### Scaling continuous ```square_feet```, ```floor_count```, ```air_temperature```\n",
    "To prevent variables with high numerical values unintentionally dominating during fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e5602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features we want to scale\n",
    "features = ['square_feet', 'floor_count', \"air_temperature\"]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform\n",
    "df[features] = scaler.fit_transform(df[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e34abd",
   "metadata": {},
   "source": [
    "### Handling missing values in ```Floor_count``` (KNN imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3320ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NaN values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333e3a1",
   "metadata": {},
   "source": [
    "Impute NaN values of ```floor_count``` with a predictive value, based on building characteristics: ```primary_use```, ```square_feet```, ```year_built```. This implementation leverage K-nearest Neighbours (KNN)\n",
    "\n",
    "runtime: approx 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb47135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track total runtime\n",
    "_start_time = datetime.now()\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare the data\n",
    "df_temp = raw_data_merged[[\"floor_count\", \"primary_use\", \"square_feet\", \"year_built\"]].copy()\n",
    "\n",
    "# Convert \"primary_use\" to numeric since KNN works with numeric data\n",
    "label_enc = LabelEncoder()\n",
    "df_temp[\"primary_use\"] = label_enc.fit_transform(df_temp[\"primary_use\"])\n",
    "\n",
    "# Apply KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=5)  # you can adjust the number of neighbors if needed\n",
    "df_temp_imputed = imputer.fit_transform(df_temp)\n",
    "\n",
    "# Replace original \"floor_count\" column with imputed data\n",
    "df[\"floor_count\"] = df_temp_imputed[:, 0]\n",
    "\n",
    "# Print execution time\n",
    "print(f\"Total runtime:  {datetime.now() - _start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d8a14c",
   "metadata": {},
   "source": [
    "### Creating ```Seasonality``` variable\n",
    "Create a new ```season``` variable through the ```timestamp``` variable by assigning months to seasons (quick and dirty implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_seasonality(df, timestamp_col='timestamp'):\n",
    "    \"\"\"\n",
    "    Adds a new column to the DataFrame capturing the seasonality (Spring, Summer, Fall, Winter) \n",
    "    based on the timestamp data from the specified column.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The input DataFrame.\n",
    "    - timestamp_col: The column name with the timestamp data. Default is 'timestamp'.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with an added 'season' column.\n",
    "    \"\"\"\n",
    "    # Convert the timestamp column to datetime\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "\n",
    "    # Extract month from the timestamp\n",
    "    df['month'] = df[timestamp_col].dt.month\n",
    "\n",
    "    # Map month to season\n",
    "    def month_to_season(month):\n",
    "        if 3 <= month <= 5:\n",
    "            return 'Spring'\n",
    "        elif 6 <= month <= 8:\n",
    "            return 'Summer'\n",
    "        elif 9 <= month <= 11:\n",
    "            return 'Fall'\n",
    "        else:\n",
    "            return 'Winter'\n",
    "\n",
    "    df['season'] = df['month'].apply(month_to_season)\n",
    "\n",
    "    # Drop the month column as it's just an intermediate column\n",
    "    df = df.drop('month', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = add_seasonality(df)\n",
    "df_no_outliers = add_seasonality(df_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cabb160",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Model Specification\n",
    "- **Linear Regression**:\n",
    "  - *Pros*: Simple, Fast\n",
    "  - *Cons*: Limited complexity, Overfitting\n",
    "  - *Assumptions/Requirements*: Linearity, Independence\n",
    "  - *Interpretability*: High (Coefficients)\n",
    "\n",
    "- **Random Forest**:\n",
    "  - *Pros*: Handles non-linearity, Feature importance\n",
    "  - *Cons*: Slow on large data, Overfitting if not tuned\n",
    "  - *Assumptions/Requirements*: No assumptions on feature scale or distribution\n",
    "  - *Interpretability*: Moderate (Feature importance)\n",
    "\n",
    "- **LASSO**:\n",
    "  - *Pros*: Feature selection, Handles multicollinearity\n",
    "  - *Cons*: Can overly simplify, Tuning required\n",
    "  - *Assumptions/Requirements*: Linearity, Independence\n",
    "  - *Interpretability*: High (Sparse coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf71a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def model_specification(df, model_type=\"linear\"):\n",
    "    # Columns you want to keep\n",
    "    columns_to_keep = [\"meter_reading\", \"square_feet\", \"air_temperature\", \n",
    "                       \"primary_use\", \"year_built\", \"site_id\", \n",
    "                       \"meter\", \"building_id\", \"floor_count\", \"season\"]\n",
    "\n",
    "    # Drop columns not in the list\n",
    "    df_pred = df[columns_to_keep]\n",
    "\n",
    "    # Drop all nan rows\n",
    "    print(df_pred.shape)\n",
    "    df_pred = df_pred.dropna(axis=0, how='any')\n",
    "    print(\"remove all nan rows: \", df_pred.shape)\n",
    "    \n",
    "    # Optionally remove 0 readings\n",
    "    # df_pred = df_pred[df_pred[\"meter_reading\"] != 0]\n",
    "    # print(\"remove all zero meter readings: \", df_pred.shape)\n",
    "\n",
    "    # One-hot encode categorical columns\n",
    "    df_encoded = pd.get_dummies(df_pred, columns=[\"primary_use\", \"site_id\", \"meter\", \"building_id\", \"season\"])\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X = df_encoded.drop(\"meter_reading\", axis=1)\n",
    "    y = df_encoded[\"meter_reading\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Choose model\n",
    "    if model_type == \"linear\":\n",
    "        model = LinearRegression()\n",
    "    elif model_type == \"lasso\":  # L1 Regularization\n",
    "        model = Lasso()\n",
    "    elif model_type == \"random_forest\":\n",
    "        model = RandomForestRegressor()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, model\n",
    "\n",
    "\n",
    "_start_time = datetime.now()\n",
    "X_train, X_test, y_train, y_test, model = model_specification(df_no_outliers, model_type=\"random_forest\")\n",
    "print(f\"Total runtime:  {datetime.now() - _start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2c813",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Performance Evaluation\n",
    "**Accuracy Metrics**\n",
    "- Mean absolute error\n",
    "- Median absolute error: included for increased metric robustness to outliers\n",
    "\n",
    "**Visualisations**\n",
    "- Residual distribution\n",
    "- Residual correlation with energy consumption volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177403d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(X_train, X_test, y_train, y_test, model):\n",
    "    # Make predictions on the training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_residuals = y_train - y_train_pred\n",
    "\n",
    "    # Calculate the mean/edian absolute errors\n",
    "    mean_abs = metrics.mean_absolute_error(y_train, y_train_pred)\n",
    "    median_abs = metrics.median_absolute_error(y_train, y_train_pred)\n",
    "    print(f\"Training:\\n- Mean Absolute Error: {mean_abs} \\n- Median Absolute Error: {median_abs}\")\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    # Calculate the mean/edian absolute errors\n",
    "    mean_abs = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    median_abs = metrics.median_absolute_error(y_test, y_pred)\n",
    "    print(f\"Test:\\n- Mean Absolute Error: {mean_abs} \\n- Median Absolute Error: {median_abs}\")\n",
    "\n",
    "    # Plot distribution\n",
    "    sns.kdeplot(residuals, fill = True, label = \"test\")\n",
    "    sns.kdeplot(train_residuals, fill = True, label = \"train\")\n",
    "    plt.xlim([-2000, 2000])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot distribution of residuals relative to numerical values of target\n",
    "    plt.scatter(y_test, residuals, alpha = 0.5, marker = \".\")\n",
    "    plt.axhline(y = 0, color = \"black\")\n",
    "    plt.xlabel(\"meter_reading (target)\")\n",
    "    plt.ylabel(\"residual\")\n",
    "    plt.show()\n",
    "    \n",
    "model_evaluation(X_train, X_test, y_train, y_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
